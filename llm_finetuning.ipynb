{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mistral\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "# Quantization, modelin ağırlığını küçültmek için kullanılır.\n",
    "from transformers import BitsAndBytesConfig\n",
    "# LoRA config\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    padding_side = \"right\",\n",
    "    add_eos_token = True\n",
    ")\n",
    "\n",
    "# Dizinin başladığını ve bittiğini belirtmek adına, dizinin başına ve sonuna tokenler eklemek için.\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NF4 = normalize float 4 | pure FP4 , quantization çeşitleri\n",
    "# Quantizationda ağıtlıklar 4 bit olarak saklanırken, hesaplamaları 16 veya 32 bit veri tipleri olarak ayarlanabilir\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant= False, #ikinci bir quantization kullanmamızı sağlar, bu da parametre başına 0.4 bitlik ek tasarrufluk sağlar\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 #daha esnek, daha fazla fine tuning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\SERKAN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e735f544314c28bc2b914580245503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_4bit = True,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"databricks/databricks-dolly-15k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(dataset_name, split=\"train[0:800]\")\n",
    "eval_dataset = load_dataset(dataset_name, split=\"train[800:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response', 'category'],\n",
       "    num_rows: 800\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which is a species of fish? Tope or Rope</td>\n",
       "      <td></td>\n",
       "      <td>Tope</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why can camels survive for long without water?</td>\n",
       "      <td></td>\n",
       "      <td>Camels use the fat in their humps to keep them...</td>\n",
       "      <td>open_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice's parents have three daughters: Amy, Jes...</td>\n",
       "      <td></td>\n",
       "      <td>The name of the third daughter is Alice</td>\n",
       "      <td>open_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was Tomoaki Komorida born?</td>\n",
       "      <td>Komorida was born in Kumamoto Prefecture on Ju...</td>\n",
       "      <td>Tomoaki Komorida was born on July 10,1981.</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Who is the founder of the Communist Party?</td>\n",
       "      <td></td>\n",
       "      <td>Lenin</td>\n",
       "      <td>open_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>What is gardening?</td>\n",
       "      <td>Gardening is the practice of growing and culti...</td>\n",
       "      <td>Gardening is laying out and caring for a plot ...</td>\n",
       "      <td>information_extraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>What are your thoughts of Michael Jackson as a...</td>\n",
       "      <td></td>\n",
       "      <td>Michael Jackson is acclaimed as the greatest p...</td>\n",
       "      <td>creative_writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>What is the largest pollutant?</td>\n",
       "      <td></td>\n",
       "      <td>Carbon dioxide (CO2) - a greenhouse gas emitte...</td>\n",
       "      <td>general_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Why do people drink coffee?</td>\n",
       "      <td></td>\n",
       "      <td>Coffee contains caffeine which is a stimulant....</td>\n",
       "      <td>general_qa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           instruction  \\\n",
       "0           When did Virgin Australia start operating?   \n",
       "1             Which is a species of fish? Tope or Rope   \n",
       "2       Why can camels survive for long without water?   \n",
       "3    Alice's parents have three daughters: Amy, Jes...   \n",
       "4                      When was Tomoaki Komorida born?   \n",
       "..                                                 ...   \n",
       "795         Who is the founder of the Communist Party?   \n",
       "796                                 What is gardening?   \n",
       "797  What are your thoughts of Michael Jackson as a...   \n",
       "798                     What is the largest pollutant?   \n",
       "799                        Why do people drink coffee?   \n",
       "\n",
       "                                               context  \\\n",
       "0    Virgin Australia, the trading name of Virgin A...   \n",
       "1                                                        \n",
       "2                                                        \n",
       "3                                                        \n",
       "4    Komorida was born in Kumamoto Prefecture on Ju...   \n",
       "..                                                 ...   \n",
       "795                                                      \n",
       "796  Gardening is the practice of growing and culti...   \n",
       "797                                                      \n",
       "798                                                      \n",
       "799                                                      \n",
       "\n",
       "                                              response                category  \n",
       "0    Virgin Australia commenced services on 31 Augu...               closed_qa  \n",
       "1                                                 Tope          classification  \n",
       "2    Camels use the fat in their humps to keep them...                 open_qa  \n",
       "3              The name of the third daughter is Alice                 open_qa  \n",
       "4           Tomoaki Komorida was born on July 10,1981.               closed_qa  \n",
       "..                                                 ...                     ...  \n",
       "795                                              Lenin                 open_qa  \n",
       "796  Gardening is laying out and caring for a plot ...  information_extraction  \n",
       "797  Michael Jackson is acclaimed as the greatest p...        creative_writing  \n",
       "798  Carbon dioxide (CO2) - a greenhouse gas emitte...              general_qa  \n",
       "799  Coffee contains caffeine which is a stimulant....              general_qa  \n",
       "\n",
       "[800 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt formatter\n",
    "\n",
    "def generate_promt(sample):\n",
    "    full_prompt = f\"\"\"<s>[INST]{sample['instruction']}\n",
    "    {f\"Here is some context: {sample['context']}\" if len(sample['context'])>0 else None}\n",
    "    [/INST] {sample['response']}</s>\n",
    "    \"\"\"\n",
    "\n",
    "    return {\"text\": full_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'When did Virgin Australia start operating?',\n",
       " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\",\n",
       " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
       " 'category': 'closed_qa'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<s>[INST]When did Virgin Australia start operating?\\n    Here is some context: Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\\n    [/INST] Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.</s>\\n    \"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_promt(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_train_dataset = train_dataset.map(generate_promt,remove_columns=list(train_dataset.features))\n",
    "generated_eval_dataset = eval_dataset.map(generate_promt,remove_columns=list(train_dataset.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<s>[INST]If I have more pieces at the time of stalemate, have I won?\\n    Here is some context: Stalemate is a situation in chess where the player whose turn it is to move is not in check and has no legal move. Stalemate results in a draw. During the endgame, stalemate is a resource that can enable the player with the inferior position to draw the game rather than lose. In more complex positions, stalemate is much rarer, usually taking the form of a swindle that succeeds only if the superior side is inattentive.[citation needed] Stalemate is also a common theme in endgame studies and other chess problems.\\n\\nThe outcome of a stalemate was standardized as a draw in the 19th century. Before this standardization, its treatment varied widely, including being deemed a win for the stalemating player, a half-win for that player, or a loss for that player; not being permitted; and resulting in the stalemated player missing a turn. Stalemate rules vary in other games of the chess family.\\n    [/INST] No. \\nStalemate is a drawn position. It doesn't matter who has captured more pieces or is in a winning position</s>\\n    \"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_train_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradyan kontrol noktası\n",
    "# bazı büyük modellerin batch size'ları 1 alınsa bile bellek sorunu çıkarabilir.\n",
    "# yapay sinir ağlarında eğitim sırasında ileri doğru giderken bir kayıp değer hesaplanır ve geriye gidildiğinde bu değer dağıtılır. İleriye doğru geçişte tüm hesaplamalar saklanırki geriye doğru gidildiğinde ağırlıklar dağıtılabilsin.\n",
    "# bu saklanan değerler bellekte yer tutar. \n",
    "# gradyan kontrol noktasında sadece stratejik değerler saklanır diğerleri saklanmaz.\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param=0\n",
    "\n",
    "    for _,param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100*trainable_params/all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low rank, eğitilecek parametrelerin sayısını ayarlar, rank yükseldikçe eğitilecek parametre sayısı artar\n",
    "# alfa, öğrenilecek ağırlıklar için ölçekleme parametresidir\n",
    "# target_modules, lorada uygulanacak lineer katmanlar\n",
    "# drop al, regülerleşme tekniği, katmanlar arasında 0.05\n",
    "# görev: diziyi tamamlama, metin üretme\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_propj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 19163136 || all params: 3771234304 || trainable%: 0.5081396289717246\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=8, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db57cfdf6b04c8b8ad383f3955d76f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1, #eğitim esnasında gradyanları biriktirir, daha küçük gruplar halinde iteratif olarak hesaplamasını sağlar\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    learning_rate=2e-4, #öğrenme oranı\n",
    "    weight_decay=0.001, #ısınma adımı : Bu parametre, modelin ağırlıklarının (weights) güncellenmesi sırasında kullanılan bir düzenleyici terimdir. Ağırlık düzenlemesi, ağırlıkların büyüklüğünü ve karmaşıklığını kontrol etmek için kullanılır.\n",
    "    max_steps=20, #maksimum adım sayısı\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SERKAN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:166: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_arguments,\n",
    "    train_dataset = generated_train_dataset,\n",
    "    eval_dataset = generated_eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eğitim aşmasında use_catch \n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0561ca6f2444fb99936e75d4752a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\SERKAN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4511.5095, 'train_samples_per_second': 0.018, 'train_steps_per_second': 0.004, 'train_loss': 1.5201166152954102, 'epoch': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=1.5201166152954102, metrics={'train_runtime': 4511.5095, 'train_samples_per_second': 0.018, 'train_steps_per_second': 0.004, 'train_loss': 1.5201166152954102, 'epoch': 0.1})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_finetuned_model = \"mistral-7b-mini-ft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SERKAN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\peft\\utils\\save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7de8387a5d4550a3944df1261637a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/601M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/serkanarslan/mistral-7b-mini-ft/commit/9b97cd38085173e87f17be3cdc8da35d5d35f9ae', commit_message='Upload model', commit_description='', oid='9b97cd38085173e87f17be3cdc8da35d5d35f9ae', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.push_to_hub(my_finetuned_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
